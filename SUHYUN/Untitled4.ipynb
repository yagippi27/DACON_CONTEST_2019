{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\workspace\\\\DACON_2019'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:\\\\workspace\\\\DACON_2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>X26</th>\n",
       "      <th>X303</th>\n",
       "      <th>X241</th>\n",
       "      <th>X435</th>\n",
       "      <th>X402</th>\n",
       "      <th>X352</th>\n",
       "      <th>X305</th>\n",
       "      <th>X350</th>\n",
       "      <th>X326</th>\n",
       "      <th>...</th>\n",
       "      <th>X283</th>\n",
       "      <th>X329</th>\n",
       "      <th>X223</th>\n",
       "      <th>X266</th>\n",
       "      <th>X20</th>\n",
       "      <th>X443</th>\n",
       "      <th>X347</th>\n",
       "      <th>X75</th>\n",
       "      <th>X107</th>\n",
       "      <th>X230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2017-07-01 00:00:00</td>\n",
       "      <td>1.187315</td>\n",
       "      <td>0.385586</td>\n",
       "      <td>0.298691</td>\n",
       "      <td>0.276657</td>\n",
       "      <td>0.190045</td>\n",
       "      <td>0.160754</td>\n",
       "      <td>0.343880</td>\n",
       "      <td>0.249761</td>\n",
       "      <td>0.070329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288141</td>\n",
       "      <td>0.326095</td>\n",
       "      <td>0.356141</td>\n",
       "      <td>0.407326</td>\n",
       "      <td>0.159644</td>\n",
       "      <td>0.227077</td>\n",
       "      <td>0.168175</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.338138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2017-07-01 01:00:00</td>\n",
       "      <td>1.186123</td>\n",
       "      <td>0.478955</td>\n",
       "      <td>0.307983</td>\n",
       "      <td>0.333324</td>\n",
       "      <td>0.193712</td>\n",
       "      <td>0.167303</td>\n",
       "      <td>0.426744</td>\n",
       "      <td>0.293231</td>\n",
       "      <td>0.072641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259911</td>\n",
       "      <td>0.297695</td>\n",
       "      <td>0.296728</td>\n",
       "      <td>0.316149</td>\n",
       "      <td>0.159288</td>\n",
       "      <td>0.259118</td>\n",
       "      <td>0.184568</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.335306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2017-07-01 02:00:00</td>\n",
       "      <td>1.168301</td>\n",
       "      <td>0.467760</td>\n",
       "      <td>0.323560</td>\n",
       "      <td>0.331767</td>\n",
       "      <td>0.200656</td>\n",
       "      <td>0.165983</td>\n",
       "      <td>0.405878</td>\n",
       "      <td>0.301436</td>\n",
       "      <td>0.076477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237175</td>\n",
       "      <td>0.261525</td>\n",
       "      <td>0.288334</td>\n",
       "      <td>0.267332</td>\n",
       "      <td>0.157027</td>\n",
       "      <td>0.267059</td>\n",
       "      <td>0.177599</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.344922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2017-07-01 03:00:00</td>\n",
       "      <td>1.163425</td>\n",
       "      <td>0.443529</td>\n",
       "      <td>0.304879</td>\n",
       "      <td>0.322123</td>\n",
       "      <td>0.189067</td>\n",
       "      <td>0.155355</td>\n",
       "      <td>0.401216</td>\n",
       "      <td>0.288693</td>\n",
       "      <td>0.082617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203332</td>\n",
       "      <td>0.207759</td>\n",
       "      <td>0.253232</td>\n",
       "      <td>0.212221</td>\n",
       "      <td>0.154466</td>\n",
       "      <td>0.251081</td>\n",
       "      <td>0.156110</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.334552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2017-07-01 04:00:00</td>\n",
       "      <td>1.398466</td>\n",
       "      <td>0.390286</td>\n",
       "      <td>0.322174</td>\n",
       "      <td>0.320922</td>\n",
       "      <td>0.195500</td>\n",
       "      <td>0.154073</td>\n",
       "      <td>0.344145</td>\n",
       "      <td>0.295861</td>\n",
       "      <td>0.074067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189208</td>\n",
       "      <td>0.174952</td>\n",
       "      <td>0.266133</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.152014</td>\n",
       "      <td>0.253930</td>\n",
       "      <td>0.143873</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.346241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8755</td>\n",
       "      <td>2018-06-30 19:00:00</td>\n",
       "      <td>1.255000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.429000</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.739000</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>0.858000</td>\n",
       "      <td>11.546000</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>3.519</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8756</td>\n",
       "      <td>2018-06-30 20:00:00</td>\n",
       "      <td>1.596000</td>\n",
       "      <td>1.692000</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>0.402000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>11.591000</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>3.331</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8757</td>\n",
       "      <td>2018-06-30 21:00:00</td>\n",
       "      <td>1.379000</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>0.435000</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.233000</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.439000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.426000</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>10.766000</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>3.416</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8758</td>\n",
       "      <td>2018-06-30 22:00:00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.463000</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.462000</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.473000</td>\n",
       "      <td>0.402000</td>\n",
       "      <td>4.263</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8759</td>\n",
       "      <td>2018-06-30 23:00:00</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.435000</td>\n",
       "      <td>0.349000</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.334000</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Time       X26      X303      X241      X435      X402  \\\n",
       "0     2017-07-01 00:00:00  1.187315  0.385586  0.298691  0.276657  0.190045   \n",
       "1     2017-07-01 01:00:00  1.186123  0.478955  0.307983  0.333324  0.193712   \n",
       "2     2017-07-01 02:00:00  1.168301  0.467760  0.323560  0.331767  0.200656   \n",
       "3     2017-07-01 03:00:00  1.163425  0.443529  0.304879  0.322123  0.189067   \n",
       "4     2017-07-01 04:00:00  1.398466  0.390286  0.322174  0.320922  0.195500   \n",
       "...                   ...       ...       ...       ...       ...       ...   \n",
       "8755  2018-06-30 19:00:00  1.255000  0.175000  0.430000  0.429000  0.169000   \n",
       "8756  2018-06-30 20:00:00  1.596000  1.692000  0.332000  0.366000  0.159000   \n",
       "8757  2018-06-30 21:00:00  1.379000  0.883000  0.435000  0.497000  0.166000   \n",
       "8758  2018-06-30 22:00:00  1.000000  0.539000  0.268000  0.459000  0.463000   \n",
       "8759  2018-06-30 23:00:00  0.976000  0.481000  0.220000  0.418000  0.646000   \n",
       "\n",
       "          X352      X305      X350      X326  ...      X283      X329  \\\n",
       "0     0.160754  0.343880  0.249761  0.070329  ...  0.288141  0.326095   \n",
       "1     0.167303  0.426744  0.293231  0.072641  ...  0.259911  0.297695   \n",
       "2     0.165983  0.405878  0.301436  0.076477  ...  0.237175  0.261525   \n",
       "3     0.155355  0.401216  0.288693  0.082617  ...  0.203332  0.207759   \n",
       "4     0.154073  0.344145  0.295861  0.074067  ...  0.189208  0.174952   \n",
       "...        ...       ...       ...       ...  ...       ...       ...   \n",
       "8755  0.739000  0.573000  0.341000  0.127000  ...  0.312000  0.381000   \n",
       "8756  0.255000  0.501000  0.281000  0.126000  ...  0.327000  0.402000   \n",
       "8757  0.233000  0.634000  0.401000  0.439000  ...  0.253000  0.401000   \n",
       "8758  0.454000  0.676000  0.543000  0.737000  ...  0.332000  0.401000   \n",
       "8759  0.419000  0.680000  0.499000  0.304000  ...  0.434000  0.435000   \n",
       "\n",
       "          X223      X266        X20      X443      X347    X75   X107  \\\n",
       "0     0.356141  0.407326   0.159644  0.227077  0.168175  0.275  0.021   \n",
       "1     0.296728  0.316149   0.159288  0.259118  0.184568  0.222  0.021   \n",
       "2     0.288334  0.267332   0.157027  0.267059  0.177599  0.237  0.021   \n",
       "3     0.253232  0.212221   0.154466  0.251081  0.156110  0.229  0.020   \n",
       "4     0.266133  0.244444   0.152014  0.253930  0.143873  0.202  0.021   \n",
       "...        ...       ...        ...       ...       ...    ...    ...   \n",
       "8755  0.479000  0.858000  11.546000  0.428000  0.134000  3.519  0.027   \n",
       "8756  0.725000  0.816000  11.591000  0.413000  0.206000  3.331  0.027   \n",
       "8757  0.426000  0.659000  10.766000  0.466000  0.324000  3.416  0.026   \n",
       "8758  0.353000  0.462000   0.312000  0.473000  0.402000  4.263  0.027   \n",
       "8759  0.349000  0.792000   0.309000  0.519000  0.334000  0.784  0.027   \n",
       "\n",
       "          X230  \n",
       "0     0.338138  \n",
       "1     0.335306  \n",
       "2     0.344922  \n",
       "3     0.334552  \n",
       "4     0.346241  \n",
       "...        ...  \n",
       "8755  0.286000  \n",
       "8756  0.303000  \n",
       "8757  0.253000  \n",
       "8758  0.253000  \n",
       "8759  0.310000  \n",
       "\n",
       "[8760 rows x 201 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('.\\\\SOOMIN\\\\data\\\\Test_시간별평균대치.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(signal_data, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(signal_data)-look_back):\n",
    "        dataX.append(signal_data[i:(i+look_back), 0])\n",
    "        dataY.append(signal_data[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "class CustomHistory(keras.callbacks.Callback):\n",
    "    def init(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Time'] = pd.to_datetime(test['Time']) \n",
    "test = test.set_index('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-9609d0ece051>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msignal_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "signal_data = test.iloc[:, 0].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "signal_data = scaler.fit_transform(signal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분리\n",
    "train = signal_data[:5256]\n",
    "val = signal_data[5256:7008]\n",
    "test = signal_data[7008:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "x_train, y_train = create_dataset(train, look_back)\n",
    "x_val, y_val = create_dataset(val, look_back)\n",
    "x_test, y_test = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "x_val = np.reshape(x_val, (x_val.shape[0], x_val.shape[1], 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "for i in range(2):\n",
    "    model.add(LSTM(32, batch_input_shape=(1, look_back, 1), stateful=True, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "model.add(LSTM(32, batch_input_shape=(1, look_back, 1), stateful=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_callback = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 모델 학습시키기\n",
    "custom_hist = CustomHistory()\n",
    "custom_hist.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 5232 samples, validate on 1728 samples\n",
      "Epoch 1/1\n",
      " 571/5232 [==>...........................] - ETA: 2:25 - loss: 0.0031"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-e59f61c10f9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcustom_hist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    model.fit(x_train, y_train, epochs=1, batch_size=batch_size, shuffle=False, callbacks=[custom_hist], validation_data=(x_val, y_val))\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making_shift_count = 25\n",
    "\n",
    "# dataset = Series_data.to_frame()\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# dataset_sc=MinMaxScaler().fit_transform(dataset)\n",
    "# dataset_sc_df = pd.DataFrame(dataset_sc, columns=dataset.columns, index = dataset.index)\n",
    "\n",
    "# for s in range(1, making_shift_count):\n",
    "#     dataset_sc_df['shift_{}'.format(s)] = dataset_sc_df[dataset.columns].shift(s)\n",
    "\n",
    "# X_dataset = dataset_sc_df.dropna().drop(dataset.columns, axis=1)\n",
    "# y_dataset = dataset_sc_df.dropna()[dataset.columns]\n",
    "\n",
    "# X_dataset = X_dataset.values\n",
    "# y_dataset = y_dataset.values\n",
    "\n",
    "# X_dataset_t = X_dataset.reshape(X_dataset.shape[0], making_shift_count-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "import keras.backend as K \n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 20)                1760      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,781\n",
      "Trainable params: 1,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "    \n",
    "model = Sequential() # Sequeatial Model \n",
    "model.add(LSTM(20, input_shape=(X_dataset_t.shape[1], 1))) # (timestep, feature) \n",
    "model.add(Dense(1)) # output = 1 \n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8736/8736 [==============================] - 7s 807us/step - loss: 0.0046 - acc: 2.2894e-04\n",
      "Epoch 2/100\n",
      " 240/8736 [..............................] - ETA: 5s - loss: 0.0032 - acc: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8736/8736 [==============================] - 6s 639us/step - loss: 0.0037 - acc: 2.2894e-04\n",
      "Epoch 3/100\n",
      "8736/8736 [==============================] - 5s 543us/step - loss: 0.0035 - acc: 2.2894e-04\n",
      "Epoch 4/100\n",
      "8736/8736 [==============================] - 5s 562us/step - loss: 0.0034 - acc: 2.2894e-04 0s - loss: 0.0035 \n",
      "Epoch 5/100\n",
      "8736/8736 [==============================] - 5s 552us/step - loss: 0.0030 - acc: 2.2894e-04\n",
      "Epoch 6/100\n",
      "8736/8736 [==============================] - 5s 534us/step - loss: 0.0025 - acc: 2.2894e-04\n",
      "Epoch 7/100\n",
      "8736/8736 [==============================] - 5s 550us/step - loss: 0.0023 - acc: 3.4341e-04 1s - loss: 0.0024 - acc: 1.7825e- - ETA: 1s - loss: 0.0024 - acc: 1.7271e - ETA: 1s - loss: 0.0024 - acc: 1.6420e-0 - ETA: 1s - loss: 0.0024 - acc: - ETA: 0s - loss: 0.0024 - acc\n",
      "Epoch 8/100\n",
      "8736/8736 [==============================] - 5s 553us/step - loss: 0.0021 - acc: 3.4341e-04\n",
      "Epoch 9/100\n",
      "8736/8736 [==============================] - 5s 552us/step - loss: 0.0020 - acc: 3.4341e-04 1s - loss: 0\n",
      "Epoch 10/100\n",
      "8736/8736 [==============================] - 5s 539us/step - loss: 0.0020 - acc: 3.4341e-04 2s - loss: 0.0019 - acc: 4.2194e-0 - ETA: 2s - loss: 0.0019 - acc:  - ETA: 1s - loss\n",
      "Epoch 11/100\n",
      "8736/8736 [==============================] - 5s 564us/step - loss: 0.0019 - acc: 3.4341e-04 2s - loss: 0.0022  - ETA: 1s - loss:\n",
      "Epoch 12/100\n",
      "8736/8736 [==============================] - 5s 553us/step - loss: 0.0019 - acc: 3.4341e-04 0s - loss: 0.0019 - acc\n",
      "Epoch 13/100\n",
      "8736/8736 [==============================] - 5s 541us/step - loss: 0.0019 - acc: 3.4341e-04 0s - loss: 0.0019 - acc: 2\n",
      "Epoch 14/100\n",
      "8736/8736 [==============================] - 5s 547us/step - loss: 0.0019 - acc: 3.4341e-04 1s - loss: 0.0018 - acc: 1.449 - ETA: 0s - loss: 0.0019 - a\n",
      "Epoch 15/100\n",
      "8736/8736 [==============================] - 5s 552us/step - loss: 0.0019 - acc: 3.4341e-04\n",
      "Epoch 16/100\n",
      "8736/8736 [==============================] - 5s 538us/step - loss: 0.0019 - acc: 3.4341e-04 1s - loss: 0.\n",
      "Epoch 17/100\n",
      "8736/8736 [==============================] - 5s 536us/step - loss: 0.0019 - acc: 3.4341e-04\n",
      "Epoch 18/100\n",
      "8736/8736 [==============================] - 5s 546us/step - loss: 0.0018 - acc: 3.4341e-04\n",
      "Epoch 19/100\n",
      "8736/8736 [==============================] - 5s 533us/step - loss: 0.0018 - acc: 3.4341e-04 1s - loss: 0.0017 - acc: 4.8544e-0 - ETA: 1s - loss: 0\n",
      "Epoch 20/100\n",
      "8736/8736 [==============================] - 5s 538us/step - loss: 0.0017 - acc: 3.4341e-04\n",
      "Epoch 21/100\n",
      "8736/8736 [==============================] - 5s 532us/step - loss: 0.0018 - acc: 3.4341e-04 0s - loss: 0.0017 - acc: \n",
      "Epoch 22/100\n",
      "8736/8736 [==============================] - 5s 557us/step - loss: 0.0017 - acc: 3.4341e-04 0s - loss: 0.0017 - acc: 2.4\n",
      "Epoch 23/100\n",
      "8736/8736 [==============================] - 5s 555us/step - loss: 0.0018 - acc: 3.4341e-04\n",
      "Epoch 24/100\n",
      "8736/8736 [==============================] - 5s 556us/step - loss: 0.0017 - acc: 3.4341e-04 0s - loss: 0.0017 - a\n",
      "Epoch 25/100\n",
      "8736/8736 [==============================] - 5s 553us/step - loss: 0.0017 - acc: 3.4341e-04\n",
      "Epoch 26/100\n",
      "8736/8736 [==============================] - 5s 586us/step - loss: 0.0017 - acc: 3.4341e-04\n",
      "Epoch 27/100\n",
      "8736/8736 [==============================] - 5s 584us/step - loss: 0.0017 - acc: 3.4341e-04 1s - loss: 0.001\n",
      "Epoch 28/100\n",
      "8736/8736 [==============================] - 5s 581us/step - loss: 0.0017 - acc: 3.4341e-04 0s - loss: 0.0017 - acc: 2.46\n",
      "Epoch 29/100\n",
      "8736/8736 [==============================] - 5s 553us/step - loss: 0.0017 - acc: 3.4341e-04 0s - loss: 0.0017\n",
      "Epoch 30/100\n",
      "8736/8736 [==============================] - 5s 564us/step - loss: 0.0017 - acc: 3.4341e-04\n",
      "Epoch 31/100\n",
      "8736/8736 [==============================] - 5s 607us/step - loss: 0.0016 - acc: 3.4341e-04\n",
      "Epoch 32/100\n",
      "8736/8736 [==============================] - 5s 550us/step - loss: 0.0016 - acc: 3.4341e-04 2s - loss: 0.0015 -  - ETA: 1s - loss: 0.0016 - ETA: 0s - loss: 0.0016 - acc: 3.610\n",
      "Epoch 33/100\n",
      "8736/8736 [==============================] - 5s 557us/step - loss: 0.0016 - acc: 3.4341e-04\n",
      "Epoch 34/100\n",
      "8736/8736 [==============================] - 5s 555us/step - loss: 0.0016 - acc: 3.4341e-04\n",
      "Epoch 35/100\n",
      "8736/8736 [==============================] - 5s 554us/step - loss: 0.0016 - acc: 3.4341e-04\n",
      "Epoch 36/100\n",
      "8736/8736 [==============================] - 5s 562us/step - loss: 0.0015 - acc: 3.4341e-04 1s - loss: 0.0015 - acc:  - ETA: 0s - loss: 0.0015 - acc: 2.5253e - ETA: 0s - loss: 0.0015 - acc: 3.636\n",
      "Epoch 37/100\n",
      "8736/8736 [==============================] - 7s 784us/step - loss: 0.0016 - acc: 3.4341e-04\n",
      "Epoch 38/100\n",
      "8736/8736 [==============================] - 5s 566us/step - loss: 0.0015 - acc: 3.4341e-04\n",
      "Epoch 39/100\n",
      "8736/8736 [==============================] - 5s 571us/step - loss: 0.0016 - acc: 3.4341e-04 0s - loss: 0.0016 - acc: 3.4722e-\n",
      "Epoch 40/100\n",
      "8736/8736 [==============================] - 5s 576us/step - loss: 0.0016 - acc: 3.4341e-04\n",
      "Epoch 41/100\n",
      "8736/8736 [==============================] - 5s 559us/step - loss: 0.0015 - acc: 3.4341e-04\n",
      "Epoch 42/100\n",
      "8736/8736 [==============================] - 5s 598us/step - loss: 0.0016 - acc: 3.4341e-04\n",
      "Epoch 43/100\n",
      "8736/8736 [==============================] - 5s 576us/step - loss: 0.0015 - acc: 3.4341e-04 - ETA: 0s - loss: 0.0015 \n",
      "Epoch 44/100\n",
      "8736/8736 [==============================] - 5s 595us/step - loss: 0.0015 - acc: 3.4341e-04 0s - loss: 0.0015 - \n",
      "Epoch 45/100\n",
      "8736/8736 [==============================] - 5s 580us/step - loss: 0.0015 - acc: 3.4341e-04\n",
      "Epoch 46/100\n",
      "8736/8736 [==============================] - 5s 579us/step - loss: 0.0015 - acc: 3.4341e-04\n",
      "Epoch 47/100\n",
      "8736/8736 [==============================] - 5s 591us/step - loss: 0.0015 - acc: 3.4341e-04\n",
      "Epoch 48/100\n",
      "8736/8736 [==============================] - 5s 585us/step - loss: 0.0015 - acc: 3.4341e-04\n",
      "Epoch 49/100\n",
      "8736/8736 [==============================] - 5s 573us/step - loss: 0.0015 - acc: 3.4341e-04\n",
      "Epoch 50/100\n",
      "8736/8736 [==============================] - 10s 1ms/step - loss: 0.0015 - acc: 3.4341e-04\n",
      "Epoch 51/100\n",
      "8736/8736 [==============================] - 5s 583us/step - loss: 0.0014 - acc: 3.4341e-04\n",
      "Epoch 52/100\n",
      "8736/8736 [==============================] - 5s 599us/step - loss: 0.0014 - acc: 3.4341e-04\n",
      "Epoch 53/100\n",
      "8736/8736 [==============================] - 5s 592us/step - loss: 0.0014 - acc: 3.4341e-04 1s \n",
      "Epoch 54/100\n",
      "8736/8736 [==============================] - 5s 585us/step - loss: 0.0015 - acc: 3.4341e-04 2s - loss: 0.001 - ETA: 1s - \n",
      "Epoch 55/100\n",
      "8736/8736 [==============================] - 5s 602us/step - loss: 0.0014 - acc: 3.4341e-04\n",
      "Epoch 56/100\n",
      "8736/8736 [==============================] - 5s 623us/step - loss: 0.0014 - acc: 3.4341e-04 1s - \n",
      "Epoch 57/100\n",
      "8736/8736 [==============================] - 7s 837us/step - loss: 0.0014 - acc: 3.4341e-04 0s - loss: 0.0014 - acc: 3. - ETA: 0s - loss: 0.0014 - acc: 3.4483e-0\n",
      "Epoch 58/100\n",
      " 750/8736 [=>............................] - ETA: 4s - loss: 0.0012 - acc: 0.0000e+00    "
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=1, verbose=1)\n",
    "\n",
    "history = model.fit(X_dataset_t, y_dataset, epochs=100, batch_size=30, verbose=1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss=history.history['val_loss'] # 오차 표시를 위해 저장\n",
    "y_acc=history.history['acc'] # 정확도 표시를 위해 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3) # 빨간색은 오차\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3) # 파란색은 정확도\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
