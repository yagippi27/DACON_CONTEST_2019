{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def mask(T=200):\n",
    "    mask = np.zeros(T)\n",
    "    indices = np.random.permutation(np.arange(T))[:2]\n",
    "    mask[indices] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def toy_problem(N=10, T=200):\n",
    "    signals = np.random.uniform(low=0.0, high=1.0, size=(N, T))\n",
    "    masks = np.zeros((N, T))\n",
    "    for i in range(N):\n",
    "        masks[i] = mask(T)\n",
    "\n",
    "    data = np.zeros((N, T, 2))\n",
    "    data[:, :, 0] = signals[:]\n",
    "    data[:, :, 1] = masks[:]\n",
    "    target = (signals * masks).sum(axis=1).reshape(N, 1)\n",
    "\n",
    "    return (data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "데이터를 생성한다\n",
    "'''\n",
    "N = 10000\n",
    "T = 200\n",
    "maxlen = T\n",
    "\n",
    "X, Y = toy_problem(N=N, T=T)\n",
    "\n",
    "N_train = int(N * 0.9)\n",
    "N_validation = N - N_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "모델을 설정한다\n",
    "'''\n",
    "n_in = len(X[0][0])  # 2\n",
    "n_hidden = 100\n",
    "n_out = len(Y[0])  # 1\n",
    "\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    return np.random.normal(scale=.01, size=shape)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=100, verbose=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_hidden,\n",
    "               kernel_initializer=weight_variable,\n",
    "               input_shape=(maxlen, n_in)))\n",
    "model.add(Dense(n_out, kernel_initializer=weight_variable))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/1000\n",
      "10000/10000 [==============================] - 19s 2ms/step - loss: 0.2944\n",
      "Epoch 2/1000\n",
      "10000/10000 [==============================] - 18s 2ms/step - loss: 0.1682\n",
      "Epoch 3/1000\n",
      "10000/10000 [==============================] - 19s 2ms/step - loss: 0.1684\n",
      "Epoch 4/1000\n",
      "10000/10000 [==============================] - 19s 2ms/step - loss: 0.1679\n",
      "Epoch 5/1000\n",
      "10000/10000 [==============================] - 19s 2ms/step - loss: 0.1680: 0s - loss: 0\n",
      "Epoch 6/1000\n",
      "10000/10000 [==============================] - 19s 2ms/step - loss: 0.1680\n",
      "Epoch 7/1000\n",
      "10000/10000 [==============================] - 19s 2ms/step - loss: 0.1681\n",
      "Epoch 8/1000\n",
      "10000/10000 [==============================] - 20s 2ms/step - loss: 0.1687\n",
      "Epoch 9/1000\n",
      "10000/10000 [==============================] - 20s 2ms/step - loss: 0.1683\n",
      "Epoch 10/1000\n",
      "10000/10000 [==============================] - 20s 2ms/step - loss: 0.1690\n",
      "Epoch 11/1000\n",
      "10000/10000 [==============================] - 21s 2ms/step - loss: 0.1686\n",
      "Epoch 12/1000\n",
      "10000/10000 [==============================] - 21s 2ms/step - loss: 0.1688\n",
      "Epoch 13/1000\n",
      "10000/10000 [==============================] - 21s 2ms/step - loss: 0.1684\n",
      "Epoch 14/1000\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.1684\n",
      "Epoch 15/1000\n",
      "10000/10000 [==============================] - 23s 2ms/step - loss: 0.1682\n",
      "Epoch 16/1000\n",
      "10000/10000 [==============================] - 23s 2ms/step - loss: 0.1692\n",
      "Epoch 17/1000\n",
      "10000/10000 [==============================] - 24s 2ms/step - loss: 0.1685\n",
      "Epoch 18/1000\n",
      "10000/10000 [==============================] - 24s 2ms/step - loss: 0.1684\n",
      "Epoch 19/1000\n",
      "10000/10000 [==============================] - 25s 3ms/step - loss: 0.1689\n",
      "Epoch 20/1000\n",
      "10000/10000 [==============================] - 26s 3ms/step - loss: 0.1689\n",
      "Epoch 21/1000\n",
      "10000/10000 [==============================] - 27s 3ms/step - loss: 0.1681\n",
      "Epoch 22/1000\n",
      "10000/10000 [==============================] - 30s 3ms/step - loss: 0.1691\n",
      "Epoch 23/1000\n",
      "10000/10000 [==============================] - 28s 3ms/step - loss: 0.1680\n",
      "Epoch 24/1000\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 0.1679\n",
      "Epoch 25/1000\n",
      "10000/10000 [==============================] - 29s 3ms/step - loss: 0.1680\n",
      "Epoch 26/1000\n",
      "10000/10000 [==============================] - 30s 3ms/step - loss: 0.1692\n",
      "Epoch 27/1000\n",
      "10000/10000 [==============================] - 30s 3ms/step - loss: 0.1681\n",
      "Epoch 28/1000\n",
      "10000/10000 [==============================] - 31s 3ms/step - loss: 0.1682\n",
      "Epoch 29/1000\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 0.1683\n",
      "Epoch 30/1000\n",
      "10000/10000 [==============================] - 32s 3ms/step - loss: 0.1681\n",
      "Epoch 31/1000\n",
      "10000/10000 [==============================] - 33s 3ms/step - loss: 0.1681\n",
      "Epoch 32/1000\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 0.1685\n",
      "Epoch 33/1000\n",
      "10000/10000 [==============================] - 35s 3ms/step - loss: 0.1681\n",
      "Epoch 34/1000\n",
      "10000/10000 [==============================] - 37s 4ms/step - loss: 0.1679\n",
      "Epoch 35/1000\n",
      "10000/10000 [==============================] - 37s 4ms/step - loss: 0.1681\n",
      "Epoch 36/1000\n",
      "10000/10000 [==============================] - 37s 4ms/step - loss: 0.1686\n",
      "Epoch 37/1000\n",
      "10000/10000 [==============================] - 38s 4ms/step - loss: 0.1680\n",
      "Epoch 38/1000\n",
      "10000/10000 [==============================] - 39s 4ms/step - loss: 0.1684\n",
      "Epoch 39/1000\n",
      "10000/10000 [==============================] - 38s 4ms/step - loss: 0.1681\n",
      "Epoch 40/1000\n",
      "10000/10000 [==============================] - 40s 4ms/step - loss: 0.1683\n",
      "Epoch 41/1000\n",
      "10000/10000 [==============================] - 42s 4ms/step - loss: 0.1680\n",
      "Epoch 42/1000\n",
      "10000/10000 [==============================] - 41s 4ms/step - loss: 0.1684\n",
      "Epoch 43/1000\n",
      "10000/10000 [==============================] - 41s 4ms/step - loss: 0.1682\n",
      "Epoch 44/1000\n",
      "10000/10000 [==============================] - 42s 4ms/step - loss: 0.1682\n",
      "Epoch 45/1000\n",
      "10000/10000 [==============================] - 41s 4ms/step - loss: 0.1683\n",
      "Epoch 46/1000\n",
      "10000/10000 [==============================] - 42s 4ms/step - loss: 0.1678\n",
      "Epoch 47/1000\n",
      "10000/10000 [==============================] - 41s 4ms/step - loss: 0.1684\n",
      "Epoch 48/1000\n",
      "10000/10000 [==============================] - 43s 4ms/step - loss: 0.1679\n",
      "Epoch 49/1000\n",
      "10000/10000 [==============================] - 43s 4ms/step - loss: 0.1679\n",
      "Epoch 50/1000\n",
      "10000/10000 [==============================] - 44s 4ms/step - loss: 0.1679\n",
      "Epoch 51/1000\n",
      "10000/10000 [==============================] - 44s 4ms/step - loss: 0.1681\n",
      "Epoch 52/1000\n",
      "10000/10000 [==============================] - 45s 4ms/step - loss: 0.1679\n",
      "Epoch 53/1000\n",
      "10000/10000 [==============================] - 45s 5ms/step - loss: 0.1682\n",
      "Epoch 54/1000\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.1683\n",
      "Epoch 55/1000\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.1681\n",
      "Epoch 56/1000\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.1685\n",
      "Epoch 57/1000\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.1686\n",
      "Epoch 58/1000\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.1684\n",
      "Epoch 59/1000\n",
      "10000/10000 [==============================] - 49s 5ms/step - loss: 0.1678\n",
      "Epoch 60/1000\n",
      "10000/10000 [==============================] - 50s 5ms/step - loss: 0.1681\n",
      "Epoch 61/1000\n",
      "10000/10000 [==============================] - 50s 5ms/step - loss: 0.1685\n",
      "Epoch 62/1000\n",
      "10000/10000 [==============================] - 50s 5ms/step - loss: 0.1683\n",
      "Epoch 63/1000\n",
      "10000/10000 [==============================] - 54s 5ms/step - loss: 0.1679\n",
      "Epoch 64/1000\n",
      "10000/10000 [==============================] - 58s 6ms/step - loss: 0.1681\n",
      "Epoch 65/1000\n",
      "10000/10000 [==============================] - 57s 6ms/step - loss: 0.1682\n",
      "Epoch 66/1000\n",
      "10000/10000 [==============================] - 52s 5ms/step - loss: 0.1679\n",
      "Epoch 67/1000\n",
      "10000/10000 [==============================] - 52s 5ms/step - loss: 0.1685\n",
      "Epoch 68/1000\n",
      "10000/10000 [==============================] - 53s 5ms/step - loss: 0.1684\n",
      "Epoch 69/1000\n",
      "10000/10000 [==============================] - 54s 5ms/step - loss: 0.1683\n",
      "Epoch 70/1000\n",
      "10000/10000 [==============================] - 55s 5ms/step - loss: 0.1685\n",
      "Epoch 71/1000\n",
      "10000/10000 [==============================] - 55s 5ms/step - loss: 0.1680\n",
      "Epoch 72/1000\n",
      "10000/10000 [==============================] - 56s 6ms/step - loss: 0.1680\n",
      "Epoch 73/1000\n",
      "10000/10000 [==============================] - 56s 6ms/step - loss: 0.1680\n",
      "Epoch 74/1000\n",
      "10000/10000 [==============================] - 57s 6ms/step - loss: 0.1681\n",
      "Epoch 75/1000\n",
      "10000/10000 [==============================] - 57s 6ms/step - loss: 0.1687\n",
      "Epoch 76/1000\n",
      "10000/10000 [==============================] - 58s 6ms/step - loss: 0.1680\n",
      "Epoch 77/1000\n",
      "10000/10000 [==============================] - 58s 6ms/step - loss: 0.1680\n",
      "Epoch 78/1000\n",
      "10000/10000 [==============================] - 59s 6ms/step - loss: 0.1680\n",
      "Epoch 79/1000\n",
      "10000/10000 [==============================] - 59s 6ms/step - loss: 0.1681\n",
      "Epoch 80/1000\n",
      "10000/10000 [==============================] - 60s 6ms/step - loss: 0.1681\n",
      "Epoch 81/1000\n",
      "10000/10000 [==============================] - 61s 6ms/step - loss: 0.1683\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 62s 6ms/step - loss: 0.1680\n",
      "Epoch 83/1000\n",
      "10000/10000 [==============================] - 63s 6ms/step - loss: 0.1682\n",
      "Epoch 84/1000\n",
      "10000/10000 [==============================] - 64s 6ms/step - loss: 0.1681\n",
      "Epoch 85/1000\n",
      "10000/10000 [==============================] - 65s 7ms/step - loss: 0.1683\n",
      "Epoch 86/1000\n",
      "10000/10000 [==============================] - 65s 6ms/step - loss: 0.1682\n",
      "Epoch 87/1000\n",
      "10000/10000 [==============================] - 75s 8ms/step - loss: 0.1682\n",
      "Epoch 88/1000\n",
      "10000/10000 [==============================] - 72s 7ms/step - loss: 0.1678\n",
      "Epoch 89/1000\n",
      "10000/10000 [==============================] - 70s 7ms/step - loss: 0.1680\n",
      "Epoch 90/1000\n",
      "10000/10000 [==============================] - 75s 8ms/step - loss: 0.1679\n",
      "Epoch 91/1000\n",
      "10000/10000 [==============================] - 74s 7ms/step - loss: 0.1681\n",
      "Epoch 92/1000\n",
      "10000/10000 [==============================] - 76s 8ms/step - loss: 0.1684\n",
      "Epoch 93/1000\n",
      "10000/10000 [==============================] - 80s 8ms/step - loss: 0.1681\n",
      "Epoch 94/1000\n",
      "10000/10000 [==============================] - 80s 8ms/step - loss: 0.1682\n",
      "Epoch 95/1000\n",
      "10000/10000 [==============================] - 82s 8ms/step - loss: 0.1684\n",
      "Epoch 96/1000\n",
      "10000/10000 [==============================] - 79s 8ms/step - loss: 0.1681\n",
      "Epoch 97/1000\n",
      "10000/10000 [==============================] - 81s 8ms/step - loss: 0.1679\n",
      "Epoch 98/1000\n",
      " 9300/10000 [==========================>...] - ETA: 5s - loss: 0.1672"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "모델을 학습시킨다\n",
    "'''\n",
    "epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "hist = model.fit(X, Y,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "학습이 진행되는 상황을 가시화한다\n",
    "'''\n",
    "loss = hist.history['loss']\n",
    "\n",
    "plt.rc('font', family='serif')\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(loss)), loss, label='loss', color='black')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()\n",
    "plt.savefig(__file__ + '.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
